<?xml version="1.0" ?>
<!DOCTYPE PubmedArticleSet PUBLIC "-//NLM//DTD PubMedArticle, 1st January 2023//EN" "https://dtd.nlm.nih.gov/ncbi/pubmed/out/pubmed_230101.dtd">
<PubmedArticleSet>
<PubmedArticle><MedlineCitation Status="Publisher" Owner="NLM" IndexingMethod="Automated"><PMID Version="1">37730823</PMID><DateRevised><Year>2023</Year><Month>09</Month><Day>20</Day></DateRevised><Article PubModel="Electronic"><Journal><ISSN IssnType="Electronic">2045-2322</ISSN><JournalIssue CitedMedium="Internet"><Volume>13</Volume><Issue>1</Issue><PubDate><Year>2023</Year><Month>Sep</Month><Day>20</Day></PubDate></JournalIssue><Title>Scientific reports</Title><ISOAbbreviation>Sci Rep</ISOAbbreviation></Journal><ArticleTitle>Comparison of the prediction accuracy of machine learning algorithms in crosslinguistic vowel classification.</ArticleTitle><Pagination><StartPage>15594</StartPage><MedlinePgn>15594</MedlinePgn></Pagination><ELocationID EIdType="doi" ValidYN="Y">10.1038/s41598-023-42818-3</ELocationID><Abstract><AbstractText>Machine learning algorithms can be used for the prediction of nonnative sound classification based on crosslinguistic acoustic similarity. To date, very few linguistic studies have compared the classification accuracy of different algorithms. This study aims to assess how well machines align with human speech perception by assessing the ability of three machine learning algorithms, namely, linear discriminant analysis (LDA), decision tree (C5.0), and neural network (NNET), to predict the classification of second language (L2) sounds in terms of first language (L1) categories. The models were trained using the first three formants and duration of L1 vowels and fed with the same acoustic features of L2 vowels. To validate their accuracy, adult L2 speakers completed a perceptual classification task. The results indicated that NNET predicted with success the classification of all L2 vowels with the highest proportion in terms of L1 categories, while LDA and C5.0 missed only one vowel each. Furthermore, NNET exhibited superior accuracy in predicting the full range of above chance responses, followed closely by LDA. C5.0 did not meet the anticipated performance levels. The findings can hold significant implications for advancing both the theoretical and practical frameworks of speech acquisition.</AbstractText><CopyrightInformation>&#xa9; 2023. Springer Nature Limited.</CopyrightInformation></Abstract><AuthorList CompleteYN="Y"><Author ValidYN="Y"><LastName>Georgiou</LastName><ForeName>Georgios P</ForeName><Initials>GP</Initials><AffiliationInfo><Affiliation>Department of Languages and Literature, University of Nicosia, Nicosia, Cyprus. georgiou.georg@unic.ac.cy.</Affiliation></AffiliationInfo><AffiliationInfo><Affiliation>Director of the University of Nicosia Phonetic Lab, Nicosia, Cyprus. georgiou.georg@unic.ac.cy.</Affiliation></AffiliationInfo></Author></AuthorList><Language>eng</Language><PublicationTypeList><PublicationType UI="D016428">Journal Article</PublicationType></PublicationTypeList><ArticleDate DateType="Electronic"><Year>2023</Year><Month>09</Month><Day>20</Day></ArticleDate></Article><MedlineJournalInfo><Country>England</Country><MedlineTA>Sci Rep</MedlineTA><NlmUniqueID>101563288</NlmUniqueID><ISSNLinking>2045-2322</ISSNLinking></MedlineJournalInfo><CitationSubset>IM</CitationSubset></MedlineCitation><PubmedData><History><PubMedPubDate PubStatus="medline"><Year>2023</Year><Month>9</Month><Day>21</Day><Hour>0</Hour><Minute>42</Minute></PubMedPubDate><PubMedPubDate PubStatus="pubmed"><Year>2023</Year><Month>9</Month><Day>21</Day><Hour>0</Hour><Minute>42</Minute></PubMedPubDate><PubMedPubDate PubStatus="received"><Year>2023</Year><Month>6</Month><Day>27</Day></PubMedPubDate><PubMedPubDate PubStatus="accepted"><Year>2023</Year><Month>9</Month><Day>14</Day></PubMedPubDate><PubMedPubDate PubStatus="entrez"><Year>2023</Year><Month>9</Month><Day>20</Day><Hour>23</Hour><Minute>59</Minute></PubMedPubDate></History><PublicationStatus>epublish</PublicationStatus><ArticleIdList><ArticleId IdType="pubmed">37730823</ArticleId><ArticleId IdType="doi">10.1038/s41598-023-42818-3</ArticleId><ArticleId IdType="pii">10.1038/s41598-023-42818-3</ArticleId></ArticleIdList><ReferenceList><Reference><Citation>Flege, J. E. Second language speech learning: Theory, findings and problems. In Speech Perception and Linguistic Experience: Theoretical and Methodological Issues (ed. Strange, W.) 233&#x2013;277 (York Press, 1995).</Citation></Reference><Reference><Citation>Flege, J. E., &amp; Bohn, O. S. (2021). The revised speech learning model (SLM-r). Second Language Speech Learning: Theoretical and Empirical Progress, 3&#x2013;83.</Citation></Reference><Reference><Citation>Best, C. T. A direct realist view of cross-language speech perception: New Directions in Research and Theory. In Speech Perception and Linguistic Experience: Theoretical and Methodological Issues (ed. Strange, W.) 171&#x2013;204 (York Press, 1995).</Citation></Reference><Reference><Citation>Best, C. T. &amp; Tyler, M. Non-native and second-language speech perception: Commonalities and complementarities. In Second language speech learning: In honor of James Emil Flege (eds Bohn, O.-S. &amp; Munro, M. J.) 13&#x2013;34 (John Benjamins, 2007).</Citation><ArticleIdList><ArticleId IdType="doi">10.1075/lllt.17.07bes</ArticleId></ArticleIdList></Reference><Reference><Citation>Escudero, P. Linguistic perception of &#x201c;similar&#x201d; L2 sounds. In Phonology in Perception (eds Boersma, P. &amp; Hamann, S.) 151&#x2013;190 (Mouton de Gruyter, 2009).</Citation><ArticleIdList><ArticleId IdType="doi">10.1515/9783110219234.151</ArticleId></ArticleIdList></Reference><Reference><Citation>Georgiou, G. P. Toward a new model for speech perception: The Universal Perceptual Model (UPM) of Second Language. Cogn. Process. 22(2), 277&#x2013;289 (2021).</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10339-021-01017-6</ArticleId><ArticleId IdType="pubmed">33591490</ArticleId></ArticleIdList></Reference><Reference><Citation>Georgiou, G. P. The acquisition of /&#x26a;/&#x2013;/i&#x2d0;/ is challenging: Perceptual and production evidence from Cypriot Greek speakers of English. Behav. Sci. 12(12), 469 (2022).</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/bs12120469</ArticleId><ArticleId IdType="pubmed">36546952</ArticleId><ArticleId IdType="pmc">9774097</ArticleId></ArticleIdList></Reference><Reference><Citation>Park, C. H. &amp; Park, H. A comparison of generalized linear discriminant analysis algorithms. Pattern Recogn. 41(3), 1083&#x2013;1097 (2008).</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.patcog.2007.07.022</ArticleId></ArticleIdList></Reference><Reference><Citation>Gyamfi, K. S., Brusey, J., Hunt, A. &amp; Gaura, E. Linear classifier design under heteroscedasticity in linear discriminant analysis. Expert Syst. Appl. 79, 44&#x2013;52 (2017).</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.eswa.2017.02.039</ArticleId></ArticleIdList></Reference><Reference><Citation>Elvin, J., Williams, D., Shaw, J. A., Best, C. T. &amp; Escudero, P. The role of acoustic similarity and non-native categorisation in predicting non-native discrimination: Brazilian Portuguese Vowels by English vs. Spanish Listeners. Languages 6(1), 44 (2021).</Citation><ArticleIdList><ArticleId IdType="doi">10.3390/languages6010044</ArticleId></ArticleIdList></Reference><Reference><Citation>Escudero, P., Simon, E. &amp; Mitterer, H. The perception of English front vowels by North Holland and Flemish listeners: Acoustic similarity predicts and explains cross-linguistic and L2 perception. J. Phon. 40(2), 280&#x2013;288 (2012).</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.wocn.2011.11.004</ArticleId></ArticleIdList></Reference><Reference><Citation>Georgiou, G. P. Speakers of different L1 dialects with acoustically proximal vowel systems present with similar nonnative speech perception abilities: Data from Greek listeners of Dutch. Speech Commun. 150, 32&#x2013;40 (2023).</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.specom.2023.05.003</ArticleId></ArticleIdList></Reference><Reference><Citation>Georgiou. Classification of English vowels in terms of Cypriot Greek categories: The role of acoustic similarity between L1 and L2 sounds. Can. J. Linguist. (2023, in press).</Citation></Reference><Reference><Citation>Gilichinskaya, Y. D. &amp; Strange, W. Perceptual assimilation of American English vowels by na&#xef;ve Russian listeners. J. Acoust. Soc. Am. 128, EL80&#x2013;EL85 (2010).</Citation><ArticleIdList><ArticleId IdType="doi">10.1121/1.3462988</ArticleId><ArticleId IdType="pubmed">20707419</ArticleId></ArticleIdList></Reference><Reference><Citation>Quinlan, R. C4.5: Programs for Machine Learning (Morgan Kaufmann, 1993).</Citation></Reference><Reference><Citation>Freund, Y. &amp; Schapire, R. E. A decision-theoretic generalization of on-line learning and an application to boosting. J. Comput. Syst. Sci. 55(1), 119&#x2013;139 (1997).</Citation><ArticleIdList><ArticleId IdType="doi">10.1006/jcss.1997.1504</ArticleId></ArticleIdList></Reference><Reference><Citation>Larose, D. T. Data mining and Predictive Analytics (Wiley, 2015).</Citation></Reference><Reference><Citation>Themistocleous, C. Dialect classification using vowel acoustic parameters. Speech Commun. 92, 13&#x2013;22 (2017).</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.specom.2017.05.003</ArticleId></ArticleIdList></Reference><Reference><Citation>Hinton, G. E. How neural networks learn from experience. Sci. Am. 267(3), 144&#x2013;151 (1992).</Citation><ArticleIdList><ArticleId IdType="doi">10.1038/scientificamerican0992-144</ArticleId><ArticleId IdType="pubmed">1502516</ArticleId></ArticleIdList></Reference><Reference><Citation>Svozil, D., Kvasnicka, V. &amp; Pospichal, J. Introduction to multi-layer feed-forward neural networks. Chemom. Intell. Lab. Syst. 39(1), 43&#x2013;62 (1997).</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/S0169-7439(97)00061-0</ArticleId></ArticleIdList></Reference><Reference><Citation>Yadav, N., Yadav, A. &amp; Kumar, M. An Introduction to Neural Network Methods for Differential Equations (Springer, 2015).</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/978-94-017-9816-7</ArticleId></ArticleIdList></Reference><Reference><Citation>Waibel, A., Hanazawa, T., Hinton, G., Shikano, K. &amp; Lang, K. J. Phoneme recognition using time-delay neural networks. IEEE Trans. Acoust. Speech Signal Process. 37, 328&#x2013;339 (1989).</Citation><ArticleIdList><ArticleId IdType="doi">10.1109/29.21701</ArticleId></ArticleIdList></Reference><Reference><Citation>Balaji, A., Haldar, A., Patil, K., Ruthvik, T. S., Valliappan, C. A., Jartarkar, M., &amp; Baths, V. EEG-based classification of bilingual unspoken speech using ANN. In 2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 1022&#x2013;1025. IEEE (2017, July).</Citation></Reference><Reference><Citation>Bataille, B. et al. Machine learning methods to improve bedside fluid responsiveness prediction in severe sepsis or septic shock: An observational study. Br. J. Anaesth. 126(4), 826&#x2013;834 (2021).</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.bja.2020.11.039</ArticleId><ArticleId IdType="pubmed">33461735</ArticleId></ArticleIdList></Reference><Reference><Citation>Boersma, P., &amp; Weenink, D. Praat: Doing phonetics by computer [Computer program] (2023). Retrieved from http://www.fon.hum.uva.nl/praat/</Citation></Reference><Reference><Citation>Kendall, T., &amp; Thomas, E.R. (2018). Vowels package. R package.</Citation></Reference><Reference><Citation>Lobanov, B. M. Classification of Russian vowels spoken by different speakers. J. Acoust. Soc. Am. 49(2B), 606&#x2013;608 (1971).</Citation><ArticleIdList><ArticleId IdType="doi">10.1121/1.1912396</ArticleId></ArticleIdList></Reference><Reference><Citation>R Core Team (2023). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/</Citation></Reference><Reference><Citation>Ripley, B., Venables, B., Bates, D. M., Hornik, K., Gebhardt, A., Firth, D. Package &#x2018;mass&#x2019;. Cran R (2023).</Citation></Reference><Reference><Citation>Kuhn, M., Weston, S., Culp, M., Coulter, N., &amp; Quinlan, R. Package &#x2018;C5.0&#x2019;. Cran R (2023).</Citation></Reference><Reference><Citation>Ripley, B. Package &#x2018;nnet&#x2019;. Cran R. (2022).</Citation></Reference><Reference><Citation>Georgiou, G. P., Giannakou, A., &amp; Alexander, K. Perception of L2 phonetic contrasts by monolinguals and bidialectals: A comparison of competencies (2023, submitted).</Citation></Reference><Reference><Citation>Byers-Gonzalez, J. M. &amp; DesJardins, S. L. Artificial Neural Networks: A new approach to predicting application behavior. Res. High. Educ. 43, 235&#x2013;258 (2002).</Citation><ArticleIdList><ArticleId IdType="doi">10.1023/A:1014423925000</ArticleId></ArticleIdList></Reference><Reference><Citation>Marshall, D. B. &amp; English, D. J. Neural network modeling of risk assessment in child protective services. Psychol. Methods 5, 102&#x2013;124 (2000).</Citation><ArticleIdList><ArticleId IdType="doi">10.1037/1082-989X.5.1.102</ArticleId><ArticleId IdType="pubmed">10937325</ArticleId></ArticleIdList></Reference><Reference><Citation>Finch, H. &amp; Schneider, M. K. Classification accuracy of neural networks vs. discriminant analysis, logistic regression, and classification and regression trees. Methodology 3(2), 47&#x2013;57 (2007).</Citation><ArticleIdList><ArticleId IdType="doi">10.1027/1614-2241.3.2.47</ArticleId></ArticleIdList></Reference><Reference><Citation>Abiodun, O. I. et al. State-of-the-art in artificial neural network applications: A survey. Heliyon 4(11), e00938 (2018).</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.heliyon.2018.e00938</ArticleId><ArticleId IdType="pubmed">30519653</ArticleId><ArticleId IdType="pmc">6260436</ArticleId></ArticleIdList></Reference><Reference><Citation>Roberts, J. B., Clayson, C. A., Robertson, F. R. &amp; Jackson, D. L. Predicting near-surface atmospheric variables from Special Sensor Microwave/Imager using neural networks with a first-guess approach. J. Geophys. Res. Atmos. 115(D19), 3099 (2010).</Citation><ArticleIdList><ArticleId IdType="doi">10.1029/2009JD013099</ArticleId></ArticleIdList></Reference><Reference><Citation>Balakrishnan, P. V., Cooper, M. C., Jacob, V. S. &amp; Lewis, P. A. A study of the classification capabilities of neural networks using unsupervised learning: A comparison with K-means clustering. Psychometrika 59, 509&#x2013;525 (1994).</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/BF02294390</ArticleId></ArticleIdList></Reference><Reference><Citation>Doupe, P., Faghmous, J. &amp; Basu, S. Machine learning for health services researchers. Value in Health 22(7), 808&#x2013;815 (2019).</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.jval.2019.02.012</ArticleId><ArticleId IdType="pubmed">31277828</ArticleId></ArticleIdList></Reference><Reference><Citation>Kotsiantis, S. B. Decision trees: A recent overview. Artif. Intell. Rev. 39, 261&#x2013;283 (2013).</Citation><ArticleIdList><ArticleId IdType="doi">10.1007/s10462-011-9272-4</ArticleId></ArticleIdList></Reference><Reference><Citation>Liu, C., Lin, B., Lai, J. &amp; Miao, D. An improved decision tree algorithm based on variable precision neighborhood similarity. Inf. Sci. 615, 152&#x2013;166 (2022).</Citation><ArticleIdList><ArticleId IdType="doi">10.1016/j.ins.2022.10.043</ArticleId></ArticleIdList></Reference><Reference><Citation>Pulverm&#xfc;ller, F. et al. Motor cortex maps articulatory features of speech sounds. Proc. Natl. Acad. Sci. 103(20), 7865&#x2013;7870 (2006).</Citation><ArticleIdList><ArticleId IdType="doi">10.1073/pnas.0509989103</ArticleId><ArticleId IdType="pubmed">16682637</ArticleId><ArticleId IdType="pmc">1472536</ArticleId></ArticleIdList></Reference><Reference><Citation>Liberman, A. M., Harris, K. S., Hoffman, H. S. &amp; Griffith, B. C. The discrimination of speech sounds within and across phoneme boundaries. J. Exp. Psychol. 54(5), 358 (1957).</Citation><ArticleIdList><ArticleId IdType="doi">10.1037/h0044417</ArticleId><ArticleId IdType="pubmed">13481283</ArticleId></ArticleIdList></Reference><Reference><Citation>Georgiou, G. P. &amp; Dimitriou, D. Perception of Dutch vowels by Cypriot Greek listeners: To what extent can listeners&#x2019; patterns be predicted by acoustic and perceptual similarity? Attent. Percept. Psychophys. https://doi.org/10.3758/s13414-023-02781-7 (2023).</Citation></Reference></ReferenceList></PubmedData></PubmedArticle></PubmedArticleSet>